# Deep Research Assistant

A comprehensive research assistant that transforms user topics into structured IEEE-format research reports using local LLM inference, web search, RAG, and real-time data agents.

## Features

- ğŸ¤– **Local LLM Integration** - Uses Ollama for query generation and synthesis
- ğŸ” **Web Search** - Tavily API for comprehensive web searches
- ğŸ“š **RAG System** - ChromaDB vector database for recent data retrieval
- ğŸŒ **Real-time Agents** - Fetch data from arXiv, Wikipedia, and news sources
- ğŸ“„ **IEEE Formatting** - Generate professional PDF and DOCX reports
- ğŸ¨ **Scientific UI** - Modern, knowledge-focused interface
- âš¡ **Real-time Progress** - Live updates during research

## Prerequisites

1. **Ollama** - Local LLM inference engine
   ```bash
   # Install Ollama from https://ollama.ai
   # Pull a model (e.g., llama3.2)
   ollama pull llama3.2
   ```

2. **Python 3.8+**

3. **Tavily API Key** - Get free API key from https://tavily.com

## Installation

1. **Clone or navigate to the project directory**
   ```bash
   cd X:\MSRIT\Py_project\deep-research-assistant
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   ```bash
   # Copy example env file
   copy .env.example .env
   
   # Edit .env and add your Tavily API key
   # TAVILY_API_KEY=your-api-key-here
   ```

## Configuration

Edit `.env` file to configure:

- `OLLAMA_MODEL` - LLM model to use (default: llama3.2)
- `TAVILY_API_KEY` - Your Tavily API key
- `RESEARCH_DEPTH` - Default research depth (quick/standard/deep)
- `MAX_QUERIES` - Maximum search queries per research
- `OUTPUT_DIR` - Directory for generated files

## Usage

1. **Start Ollama** (if not running)
   ```bash
   ollama serve
   ```

2. **Run the application**
   ```bash
   python main.py
   ```

3. **Open browser**
   ```
   http://localhost:5000
   ```

4. **Conduct research**
   - Enter your research topic
   - Select output format (Screen/PDF/DOCX/Both)
   - Choose research depth
   - Click "Start Research"
   - Watch real-time progress
   - View results or download files

## Project Structure

```
deep-research-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ llm/              # LLM integration (Ollama)
â”‚   â”œâ”€â”€ search/           # Web search (Tavily)
â”‚   â”œâ”€â”€ rag/              # Vector database (ChromaDB)
â”‚   â”œâ”€â”€ agents/           # Real-time data agents
â”‚   â”œâ”€â”€ synthesis/        # Research engine & citations
â”‚   â””â”€â”€ export/           # PDF/DOCX generators
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ css/          # Scientific theme
â”‚   â”‚   â””â”€â”€ js/           # Frontend logic
â”‚   â””â”€â”€ templates/        # HTML templates
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ ieee_format/      # IEEE specifications
â”œâ”€â”€ config.py             # Configuration
â”œâ”€â”€ main.py               # Flask application
â””â”€â”€ requirements.txt      # Dependencies
```

## API Endpoints

- `GET /` - Main application
- `GET /api/research/stream` - SSE stream for research progress
- `POST /api/download/<format>` - Download generated files
- `GET /api/health` - Health check
- `GET /api/stats` - RAG statistics

## Research Process

1. **Query Generation** - LLM generates search queries from topic
2. **Web Search** - Multi-query search using Tavily
3. **Real-time Data** - Fetch from arXiv, Wikipedia, news
4. **Source Scoring** - Rank by relevance and credibility
5. **RAG Indexing** - Store in vector database
6. **Content Analysis** - LLM analyzes each source
7. **Synthesis** - Generate coherent research report
8. **Citation** - Add IEEE-format citations
9. **Export** - Generate PDF/DOCX with IEEE formatting

## Output Formats

### PDF
- IEEE two-column format (configurable)
- Proper section numbering
- Bibliography with citations
- Professional typography

### DOCX
- IEEE-compliant styling
- Editable format
- Compatible with Microsoft Word
- Easy to customize

### On-Screen
- Formatted HTML display
- Interactive citations
- Responsive design
- Copy-paste friendly

## Troubleshooting

### Ollama not available
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```

### Tavily API errors
- Verify API key in `.env`
- Check API quota at https://tavily.com
- Try with fewer queries (quick mode)

### Import errors
```bash
# Reinstall dependencies
pip install -r requirements.txt --upgrade
```

### ChromaDB issues
```bash
# Clear vector database
rm -rf data/chromadb
```

## Advanced Configuration

### Custom LLM Model
```env
OLLAMA_MODEL=mistral
# or
OLLAMA_MODEL=llama2
```

### Research Depth
- **Quick**: 3 queries, ~1 minute
- **Standard**: 5 queries, ~2 minutes
- **Deep**: 8 queries, ~3 minutes

### Vector Database
- Automatic document chunking
- Semantic search
- Persistent storage
- Configurable embedding model

## Contributing

This is a demonstration project. Feel free to:
- Add more data sources
- Improve synthesis prompts
- Enhance UI design
- Add export formats
- Optimize performance

## License

MIT License - Feel free to use and modify

## Credits

Built with:
- Ollama - Local LLM inference
- Tavily - AI-powered search
- ChromaDB - Vector database
- Flask - Web framework
- ReportLab - PDF generation
- python-docx - DOCX generation

---

**Happy Researching! ğŸ”¬**
